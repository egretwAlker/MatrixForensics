%TODO: Include handy diagram from https://explained.ai/matrix-calculus/index.html

\chapter{Derivatives}

\section{Useful Rules for Derivatives}
For general $\mA$ and $\mX$ (no special structure):
\begin{align}
\partial\mA           &= 0~~\textrm{where $\mA$ is a constant} \\
\partial(c\mX)        &= c\partial\mX                          \\
\partial(\mX+\mY)     &= \partial\mX+\partial\mY               \\
\partial(\trace(\mX)) &= \trace(\partial(\mX))                 \\
\partial(\mX\mY)      &= (\partial\mX)\mY + \mX(\partial\mY)   \\
\partial(\mX\circ\mY) &= (\partial\mX)\circ\mY + \mX\circ(\partial\mY) \\
%TODO Kronecker x in circle equation 39 Matrix Cookbook
\partial(\mX^{-1})    &= -\mX^{-1}(\partial\mX)\mX^{-1}        \\
\partial(\det(\mX))   &= \trace(\textrm{adj}(\mX)\partial\mX)  \\
\partial(\det(\mX))   &= \det(\mX)\trace(\mX^{-1}\partial\mX)  \\
\partial(\ln(\det(\mX))) &= \trace(\mX^{-1}\partial\mX)        \\
\partial(\mXT)       &= (\partial\mX)\T                       \\
\partial(\mX^H)       &= (\partial\mX)^H
\end{align}

\section{Gradient Notation}
For a matrix $\mA\in\sRnm$, the gradient is defined as:
\begin{equation}
\grad_\mA f(\mA)=
\begin{bmatrix}
\pd{f(\mA)}{\mA_{11}} & \pd{f(\mA)}{\mA_{12}} & \ldots & \pd{f(\mA)}{\mA_{1m}} \\
\pd{f(\mA)}{\mA_{21}} & \pd{f(\mA)}{\mA_{22}} & \ldots & \pd{f(\mA)}{\mA_{2m}} \\
\vdots                & \vdots                & \ddots & \vdots                \\
\pd{f(\mA)}{\mA_{n1}} & \pd{f(\mA)}{\mA_{n2}} & \ldots & \pd{f(\mA)}{\mA_{nm}}
\end{bmatrix}
\end{equation}
i.e.
\begin{equation}
(\grad_\mA f(\mA))_{ij}=\pd{f(\mA)}{\mA_{ij}}
\end{equation}
Note that the size of the gradient is always the same size as the entity to which it is taken. Also note that the gradient of a function is only defined if the function is real-valued, that is, if it returns a scalar value.

\section{Derivatives of Matrices and Vectors}

\subsection{First-Order}

In the following, $\mJ$ is the Single-Entry Matrix (\autoref{sec:rogue_single_entry}).
\begin{align}
\pd{\vx\T \va}{\vx}     &= \pd{\vaT \vx}{\vx} = \va          \\
\pd{\va\T\mX\vb}{\mX}   &= \va\vbT                           \\
\pd{\va\T\mXT\vb}{\mX}  &= \vb\vaT                           \\
\pd{\va\T\mX\va}{\mX}   &= \pd{\vaT\mXT\va}{\mX} = \va\vaT \\
\pd{\mX}{\mX_{ij}}      &= \mJ^{ij}                              %TODO: What is this? (MCB 73)
%TODO: MCB 74, 75
\end{align}

\section{Derivatives of vector norms}

\begin{align}
\pd{}{\vx}\norm{\vx-\va}_2 &= \frac{\vx-\va}{\norm{\vx-\va}_2} \\
\pd{}{\vx}\frac{\vx-\va}{\norm{\vx-\va}_2} &= \frac{\mI}{\norm{\vx-\va}_2}-\frac{(\vx-\va)(\vx-\va)\T}{\norm{\vx-\va}_2^3} \\
\pd{\norm{\vx}_2^2}{\vx} &= \pd{\norm{\vxT\vx}_2}{\vx} = 2\vx
\end{align}

\section{Scalar by Vector}
\begin{center}
\begin{tabular}{l|Sc|Sc|Sc}
Qualifier                  & Expression                                    & Numerator layout                                   & Denominator layout                            \\
                           & $\pd{a}{x}$                                   & $\vzero\T$                                         & $\vzero$                                      \\
                           & $\pd{au(\vx)}{\vx}$                           & $a\pd{u}{\vx}$                                     & Same                                          \\
                           & $\pd{u(\vx)+v(\vx)}{\vx}$                     & $\pd{u}{\vx} + \pd{v}{\vx}$                        & Same                                          \\
                           & $\pd{u(\vx)v(\vx)}{\vx}$                      & $u\pd{v}{\vx} + v\pd{u}{\vx}$                      & Same                                          \\
                           & $\pd{g(u(\vx))}{\vx}$                         & $\pd{g(u)}{u}\pd{u}{\vx}$                          & Same                                          \\
                           & $\pd{f(g(u(\vx)))}{\vx}$                      & $\pd{f(g)}{g}\pd{g(u)}{u}\pd{u}{\vx}$              & Same                                          \\
                           & $\pd{\vu(\vx)\T\vv(\vx)}{\vx}$                & $\vuT\pd{\vv}{\vx}+\vvT\pd{\vu}{\vx}$              & $\pd{\vu}{\vx}\vv+\pd{\vv}{\vx}\vu$           \\
                           & $\pd{\vu(\vx)\T\mA\vv(\vx)}{\vx}$             & $\vuT\mA\pd{\vv}{\vx}+\vvT\mAT\pd{\vu}{\vx}$       & $\pd{\vu}{\vx}\mA\vv+\pd{\vv}{\vx}\mAT\vu$    \\
                           & $\md{f}{2}{\vx}{}{{\vxT}}{}$                  &                                                    & $\mH$, the Hessian matrix                     \\
                           & $\pd{\va\cdot\vx}{\vx}=\pd{\vx\cdot\va}{\vx}$ & $\vaT$                                             & $\va$                                         \\
                           & $\pd{\vbT\mA\vx}{\vx}$                        & $\vbT\mA$                                          & $\mAT\vb$                                     \\
                           & $\pd{\vxT\mA\vx}{\vx}$                        & $\vxT(\mA+\mAT)$                                   & $(\mA+\mAT)\vx$                               \\
$\mA$ symmetric            & $\pd{\vxT\mA\vx}{\vx}$                        & $2\vxT\mA$                                         & $2\mA\vx$                                     \\
                           & $\pd{\vxT\mA\vx}{\vx}$                        & $\mA+\mAT$                                         & Same                                          \\
$\mA$ symmetric            & $\pd{\vxT\mA\vx}{\vx}$                        & $\mA$                                              & Same                                          \\
                           & $\pd{\vxT\vx}{\vx}$                           & $2\vxT$                                            & $2\vx$                                        \\
                           & $\pd{\vaT\vu(\vx)}{\vx}$                      & $\vaT\pd{\vu}{\vx}$                                & $\pd{\vu}{\vx}\va$                            \\
                           & $\pd{\vaT\vx\vxT\vb}{\vx}$                    & $\vxT(\va\vbT+\vb\vaT)$                            & $(\va\vbT+\vb\vaT)\vx$                        \\
                           & $\pd{(\mA\vx+\vb)\T\mC(\mD\vx+\ve)}{\vx}$     & $(\mD\vx+\ve)\T\mCT\mA+(\mA\vx+\vb)\T\mC\mD$       & $\mDT\mCT(\mA\vx+\vb)+\mAT\mC(\mD\vx+\ve)$    \\
                           & $\pd{\norm{\vx-\va}}{\vx}$                    & $\frac{(\vx-\va)\T}{\norm{\vx-\va}}$               & $\frac{\vx-\va}{\norm{\vx-\va}}$              \\
\end{tabular}
\end{center}

\section{Vector by Vector}
\begin{center}
\begin{tabular}{l|Sc|Sc|Sc}
Qualifier                  & Expression                                    & Numerator layout                                   & Denominator layout                            \\
                           & $\pd{\va}{\vx}$                               & $\vzero$                                           & Same                                          \\ %TODO: really the same? shouldn't be transposed?
                           & $\pd{\vx}{\vx}$                               & $\mI$                                              & Same                                          \\ %TODO: Really the identity matrix? Which one?
                           & $\pd{\mA\vx}{\vx}$                            & $\mA$                                              & $\mAT$                                        \\
                           & $\pd{\vxT\mA}{\vx}$                           & $\mAT$                                             & $\mA$                                         \\
                           & $\pd{a\vu(\vx)}{\vx}$                         & $a\pd{\vu}{\vx}$                                   & Same                                          \\
                           & $\pd{a(\vx)\vu(\vx)}{\vx}$                    & $a\pd{\vu}{\vx}+\vu\pd{a}{\vx}$                    & $a\pd{\vu}{\vx}+\pd{a}{\vx}\vuT$              \\
                           & $\pd{\mA\vu(\vx)}{\vx}$                       & $\mA\pd{\vu}{\vx}$                                 & $\pd{\vu}{\vx}\mAT$                           \\
                           & $\pd{(\vu(\vx)+\vv(\vx))}{\vx}$               & $\pd{\vu}{\vx}+\pd{\vv}{\vx}$                      & Same                                          \\
                           & $\pd{\vg(\vu(\vx))}{\vx}$                     & $\pd{\vg(\vu)}{\vu}\pd{\vu}{\vx}$                  & $\pd{\vu}{\vx}\pd{\vg(\vu)}{\vu}$             \\
                           & $\pd{\vf(\vg(\vu(\vx)))}{\vx}$                & $\pd{\vf(\vg)}{\vg(\vu)}\pd{\vg(\vu)}{\vu}\pd{\vu}{\vx}$& $\pd{\vu}{\vx}\pd{\vg(\vu)}{\vu}\pd{\vf(\vg)}{\vg}$
\end{tabular}
\end{center}



\section{Matrix by Scalar}
\begin{center}
\begin{tabular}{l|Sc|Sc}
Qualifier                  & Expression                                     & Numerator layout                                       \\
                           & $\pd{a\mU(x)}{x}$                              & $a\pd{\mU}{x}$                                         \\
                           & $\pd{\mA\mU(x)\mB}{x}$                         & $\mA\pd{\mU}{x}\mB$                                    \\
                           & $\pd{(\mU(x)+\mV(x))}{x}$                      & $\pd{\mU}{x}+\pd{\mV}{x}$                              \\
                           & $\pd{(\mU(x)\mV(x))}{x}$                       & $\mU\pd{\mV}{x}+\pd{\mU}{x}\mV$                        \\
                           & $\pd{(\mU(x)\kp\mV(x))}{x}$                    & $\mU\kp\pd{\mV}{x} + \pd{\mU}{x}\kp\mV$                \\
                           & $\pd{(\mU(x)\hp\mV(x))}{x}$                    & $\mU\hp\pd{\mV}{x} + \pd{\mU}{x}\hp\mV$                \\
                           & $\pd{\mU^{-1}(x)}{x}$                          & $-\mU^{-1} \pd{\mU}{x} \mU^{-1}$                       \\
                           & $\md{\mU^{-1}}{2}{x}{}{y}{}$                   & $\mU^{-1}\left(\pd{\mU}{x}\mU^{-1}\pd{\mU}{y} - \md{\mU}{2}{x}{}{y}{} + \pd{\mU}{y}\mU^{-1}\pd{\mU}{x}\right)\mU^{-1}$ \\
                           & $\pd{e^{x\mA}}{x}$                             & $\mA e^{x\mA}=e^{x\mA}\mA$
\end{tabular}
\end{center}

\section{Traces}
\begin{align}
\pd{}{\mX}\trace(\mX)             &=\mI            \\
\pd{}{\mX}\trace(\mX\mA)          &=\mAT           \\
\pd{}{\mX}\trace(\mA\mX)          &=\mAT           \\
\pd{}{\mX}\trace(\mA\mX\mB)       &=\mAT\mBT       \\
\pd{}{\mX}\trace(\mA\mXT\mB)      &=\mB\mA         \\
\pd{}{\mX}\trace(\mXT\mA)         &=\mA            \\
\pd{}{\mX}\trace(\mA\mXT)         &=\mA            \\
\pd{}{\mX}\trace(\mA\kp\mX)       &=\trace(\mA)\mI
\end{align}
For traces with many instances of $\mX$ we can apply an analogue of the product rule. For example:
\begin{equation}
\pd{}{\mX}\trace(\mA\mX\mB\mX\mCT)=\pd{}{\mX}\trace(\mA\mX\mD)+\pd{}{\mX}\trace(\mE\mX\mCT)=\mAT\mDT+\mET\mC
\end{equation}
where $\mD=\mB\mX\mCT$ and $\mE=\mA\mX\mB$.

\section{Determinants}

\subsection{By Scalars}

If $\mX$ and $\mY$ are matrices with no special structure and $x$ is a scalar, then:

\begin{align}
\pd{\det(\mY)}{x} &= \det(\mY) \trace\left(\mYi \pd{\mY}{x}\right) \\
\sum_k \pd{\det(\mX)}{\mX_{ik}}\mX_{jk} &= \delta_{ij} \det(\mX)   \\
\pd[2]{\mY}{x^2} &= \det(\mY) %TODO: Can this be simplified with squares?
  \left(
      \trace \left(\mYi \pd{\pd{\mY}{x}}{x}\right)
    + \trace \left(\mYi\pd{\mY}{x}\right) \trace\left(\mYi\pd{\mY}{x}\right)
    - \trace \left( \left(\mYi\pd{\mY}{x}\right) \left(\mYi\pd{\mY}{x}\right) \right)
  \right)
\end{align}

\subsection{Linear forms}

\begin{align}
\pd{\det(\mX)}{\mX} &= \det(\mX)(\mXi)\T \\
\sum_k \pd{\det(\mX)}{\mX_{ik}}\mX_{jk} &= \delta_{ij} \det(\mX)   \\
\pd{\det(\mA\mX\mB)}{\mX} &= \det(\mA\mX\mB)(\mXi)\T \\
                          &= \det(\mA\mX\mB)(\mXT)^{-1}
\end{align}

\subsection{Square forms}

If $\mX$ is square and invertible:
\begin{equation}
\pd{\det(\mXT\mA\mX)}{\mX} = 2 \det(\mXT \mA \mX)\mXiT
\end{equation}
If $\mX$ is not square and $\mA$ is symmetric, then
\begin{equation}
\pd{\det(\mXT \mA \mX)}{\mX} = 2 \det(\mXT\mA\mX)\mA\mX(\mXT\mA\mX)^{-1}
\end{equation}
If $\mX$ is not square and $\mA$ is not symmetric, then
\begin{equation}
\pd{\det(\mXT\mA\mX)}{\mX} = \det(\mXT\mA\mX)\left(\mA\mX(\mXT\mA\mX)^{-1}+\mAT\mX(\mXT\mAT\mX)^{-1}\right)
\end{equation}

\subsection{Nonlinear Forms}
\begin{align}
\pd{\ln\det(\mXT\mX)}{\mX}         &= 2(\mXp)\T \\
\pd{\ln\det(\mXT\mX)}{\mXp}        &= -2\mXT    \\
\pd{\ln\lvert\det(\mX)\rvert}{\mX} &= \mXiT     \\
\pd{\det(\mX^k)}{\mX}              &= k \det(\mX^k) \mXiT
\end{align}